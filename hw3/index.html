<html>
  <head>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default"></script>
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap"
      rel="stylesheet"
    />
    <style>
      h1 {
        text-align: center;
      }

      .container {
        margin: 0 auto;
        padding: 60px 20%;
      }

      figure {
        text-align: center;
      }

      img {
        display: inline-block;
      }

      h1 {
        color: rgb(255, 255, 255);
      }

      h2 {
        color: rgb(255, 255, 255);
      }

      h3 {
        color: rgb(171, 241, 255);
      }

      body {
        font-family: "Inter", sans-serif;
        text-align: center;
        color: #f3d0f5;
        background-color: #000000;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <h1>CS184/284A Spring 2025 Homework 3 Write-Up</h1>
      <div style="text-align: center">Names: Ruben Gonzalez</div>

      <br />

      Link to webpage:
      <a
        href="https://cal-cs184.github.io/hw-webpages-su25-rgonzy/hw3/index.html"
        >WebPage</a
      >
      Link to GitHub repository:
      <a href="https://github.com/cal-cs184/hw-pathtracer-updated-gonzyr.git"
        >Repo</a
      >

      <!-- <figure>
        <img
          src="cornell.png"
          alt="Cornell Boxes with Bunnies"
          style="width: 70%"
        />
        <figcaption>You can add images with captions!</figcaption>
      </figure> -->

      <!--
		We've already added one heading per part, to make your write-up as navigable when grading. Please fit your write-up within these sections!
		-->

      <h2>Overview</h2>
      Give a high-level overview of what you implemented in this homework. Think
      about what you've built as a whole. Share your thoughts on what
      interesting things you've learned from completing the homework.

      <h2>Part 1: Ray Generation and Scene Intersection</h2>

      <p>
        To generate rays, I first convert the sensor sample coordinates into
        camera space by linearly interpolating between the camera-space edges.
        Next, for each pixel, I apply anti-aliasing by jittering the sample
        position and normalizing it using the dimensions of the sample buffer. I
        then generate a ray from the camera origin through this normalized
        point, call <code>est_radiance_global_illumination</code> along that
        ray, and add the result to a running sum. After tracing all samples, I
        divide the sum by the number of samples to obtain the final pixel color.
      </p>
      <p>
        With the basic ray tracer in place, I extended it to handle triangles by
        first testing the ray against the triangle's bounding box and, if that
        succeeds, running the Möller-Trumbore intersection. I begin by computing
        the two edge vectors E₁ and E₂ of the triangle and then form the cross
        product P between the ray direction D and E₂. I take the dot product of
        E₁ and P to get the determinant det; if det is nearly zero, the ray is
        effectively parallel to the triangle and I immediately return false.
        Otherwise, I subtract the triangle's first vertex A from the ray origin
        O to form T, use T⋅P / det to obtain the first barycentric coordinate u,
        and return false if u lies outside [0, 1]. Next, I form Q as the cross
        product of T and E₁, compute the second barycentric coordinate v as D⋅Q
        / det, and again bail out if v falls outside [0, 1] or if u + v exceeds
        1. Finally, I compute the ray parameter t by taking E₂⋅Q / det, and if t
        is outside the ray's allowed range I return false; otherwise I update
        ray.max_t to t, record the primitive ID and BSDF, compute the surface
        normal by area-weighted interpolation of the triangle's vertex normals
        using u and v, store that in the intersection, and return true.
        <br />
        <br />
        The sphere intersection follows a similar pattern. I compute the
        coefficients a, b, and c for the quadratic equation, then check whether
        the discriminant (b*b - 4*a*c) is non-negative. If it is, I solve for
        the two roots and verify whether either lies between ray.min_t and
        ray.max_t. When a valid root is found, I set ray.max_t to the smaller
        root, compute the surface normal as the normalized vector from the
        sphere;s center to the hit point, fill the intersection record
        (primitive ID, BSDF, and this normal), and return true.
      </p>

      <p>
        In the photos below, you can see the result of my pathtracer up to this
        point.
      </p>

      <div style="display: flex; flex-direction: column; align-items: center">
        <table
          style="width: 100%; text-align: center; border-collapse: collapse"
        >
          <tr>
            <td>
              <img src="part1.1.png" width="400px" />
              <figcaption>Spheres</figcaption>
            </td>
            <td>
              <img src="part1.2.png" width="400px" />
              <figcaption>Coil</figcaption>
            </td>
          </tr>
          <tr>
            <img src="part1.3.png" width="400px" />
            <figcaption>Bunny</figcaption>
          </tr>
        </table>
      </div>

      <!-- <p>
        Here is an example 2x2 gridlike structure using an HTML table. Each
        <b>tr</b> is a row and each <b>td</b> is a column in that row. You might
        find this useful for framing and showing your result images in an
        organized fashion.
      </p>
      <div style="display: flex; flex-direction: column; align-items: center">
        <table
          style="width: 100%; text-align: center; border-collapse: collapse"
        >
          <tr>
            <td style="text-align: center">
              <img src="cornell.png" width="400px" />
              <figcaption>Caption goes here.</figcaption>
            </td>
            <td style="text-align: center">
              <img src="cornell.png" width="400px" />
              <figcaption>Caption goes here.</figcaption>
            </td>
          </tr>
          <tr>
            <td style="text-align: center">
              <img src="cornell.png" width="400px" />
              <figcaption>Caption goes here.</figcaption>
            </td>
            <td style="text-align: center">
              <img src="cornell.png" width="400px" />
              <figcaption>Caption goes here.</figcaption>
            </td>
          </tr>
        </table>
      </div> -->

      <h2>Part 2: Bounding Volume Hierarchy</h2>
      <p>
        To build the BVH, I begin by initializing the structure and allocating
        space for every scene primitive. I then create the root node and
        recursively subdivide its primitives until each leaf contains at most
        max_leaf_size items. At each split, I choose the longest axis of the
        node's bounding box by comparing its diagonal's x, y, and z components.
        Along that axis, I form a small bounding box that encloses only the
        primitives' centroids and pick the split plane at its midpoint. I
        partition the primitives into two groups based on which side of this
        plane their centroid falls on and pass the midpoint iterator into the
        left and right recursive calls. Finally, to avoid edge cases when all
        centroids lie on one side, I fall back to dividing the list evenly in
        half.
      </p>

      <figure>
        <img src="part2.4.png" width="500px" />
        <figcaption>Cow generated using BVH</figcaption>
      </figure>

      <figure>
        <img src="part2.3.png" width="500px" />
        <figcaption>Dragon generated using BVH</figcaption>
      </figure>

      <p>
        The speed-up from BVH is an incredible feat. Below are some time
        comparisons.
      </p>

      <figure>
        <img src="part2.1.png" width="800px" />
        <figcaption>CBLucy generated in .2045s compared to 52s</figcaption>
      </figure>

      <figure>
        <img src="part2.2.png" width="500px" />
        <figcaption>Beast generated in .1375s compared to 37s</figcaption>
      </figure>

      <figure>
        <img src="part2.5.png" width="500px" />
        <figcaption>Walle generated in .3492s compared to 83s</figcaption>
      </figure>

      <p>
        Rendering times were significantly reduced when using BVH acceleration
        compared to rendering without it. In scenes with moderately complex
        geometries, the version without BVH had to check every object for each
        ray, resulting in noticeably longer render times. With BVH, objects are
        grouped into bounding boxes within a tree structure, allowing the
        renderer to skip entire groups of objects when a ray misses a bounding
        box. This reduces the number of intersection tests from O(N) to
        approximately O(log N), leading to a substantial performance
        improvement. The difference in times clearly demonstrates the
        effectiveness of BVH in optimizing ray tracing workloads.
      </p>

      <h2>Part 3: Direct Illumination</h2>

      <p>
        To estimate direct lighting, I implemented two functions: one that
        samples directions uniformly over the hemisphere, and another that
        samples only from light sources. The hemisphere-based function generates
        a total of lights * ns_area_light samples. For each sample, it creates a
        ray originating from the surface hit point and directed toward a
        randomly chosen point on the hemisphere above the surface normal. If
        this ray intersects another object, the incoming radiance from that
        direction is computed using the BSDF and the emission at the
        intersection point. This contribution is added to a running sum. After
        all samples are processed, the accumulated radiance is divided by the
        number of samples and the probability density function (PDF) for uniform
        hemisphere sampling to produce the final estimate.
      </p>

      <p>
        Direct lighting samples points on light sources and casts shadow rays
        from the hit point to these sampled light positions. If the shadow ray
        reaches the light unblocked, it evaluates the rendering equation for
        that light contribution. This approach is efficient because it only
        samples directions that potentially contribute light, unlike hemisphere
        sampling which may waste samples on directions with no illumination. The
        final result properly weights each sample by its probability density to
        maintain an unbiased estimate.
      </p>

      <div style="display: flex; flex-direction: column; align-items: center">
        <table
          style="width: 100%; text-align: center; border-collapse: collapse"
        >
          <tr>
            <td>
              <img src="bunnyHemiSphere.png" width="400px" />
              <figcaption>Bunny Generated with Hemisphere sampling</figcaption>
            </td>
            <td>
              <img src="DragonHemiSphere.png" width="400px" />
              <figcaption>Dragon Generated with Hemisphere sampling</figcaption>
            </td>
          </tr>
          <tr>
            <td>
              <img src="walle_64_32.png" width="400px" />
              <figcaption>Walle generated with Light sampling</figcaption>
            </td>
            <td>
              <img src="dragon_64_32.png" width="400px" />
              <figcaption>Dragon generated with Light sampling</figcaption>
            </td>
          </tr>
        </table>
      </div>

      <p>
        In the table below, I generated a bench with varying numbers of rays per
        light. When only one ray was shot, the shadow underneath the bench was
        almost completely absent. The situation remained similar until reaching
        16 samples, where the shadow became more noticeable. At 64 samples, the
        shadow was clearly visible. This difference is due to the noise caused
        by the limited number of light rays.
      </p>

      <div style="display: flex; flex-direction: column; align-items: center">
        <table
          style="width: 100%; text-align: center; border-collapse: collapse"
        >
          <tr>
            <td>
              <img src="bench_1_1.png" width="400px" />
              <figcaption>
                Bench with 1 light ray and 1 sample per pixel
              </figcaption>
            </td>
            <td>
              <img src="bench_1_4.png" width="400px" />
              <figcaption>
                Bench with 4 light rays and 1 sample per pixel
              </figcaption>
            </td>
          </tr>
          <tr>
            <td>
              <img src="bench_1_16.png" width="400px" />
              <figcaption>
                Bench with 16 light rays and 1 sample per pixel
              </figcaption>
            </td>
            <td>
              <img src="bench_1_64.png" width="400px" />
              <figcaption>
                Bench with 64 light rays and 1 sample per pixel
              </figcaption>
            </td>
          </tr>
        </table>
      </div>

      <p>
        Hemisphere sampling uses random directions (often cosine-weighted)
        across the hemisphere above a surface point. While unbiased, it wastes
        samples on directions where little light arrives. Importance sampling
        strategically biases the random sampling toward directions that
        contribute more to the final result - like toward light sources or based
        on the BRDF. This reduces noise because more samples hit areas that
        actually matter for the lighting calculation. It's still random, but the
        randomness is weighted to be more effective. Both methods eventually
        converge to the correct result, but importance sampling typically needs
        fewer samples to achieve the same quality.
      </p>

      <h2>Part 4: Global Illumination</h2>
      <p>
        To achieve global (indirect) illumination, I implemented a recursive
        path tracing algorithm that begins with the first bounce. After
        computing the initial bounce, I used Russian roulette to
        probabilistically terminate the path. If the path continues, I sample
        the BSDF at the intersection point to generate a new ray direction and
        trace it further. If this new ray intersects another object, the process
        repeats recursively until the maximum ray depth is reached or the path
        is terminated. At each step, the incoming radiance is weighted by the
        BSDF and accumulated, forming a running sum that approximates the total
        indirect lighting contribution.
      </p>

      <figure>
        <img src="blob1024_16.png" width="500px" />
        <figcaption>
          Blob generated with max_ray_depth = 5, 1024 samples per pixel, and 16
          rays per light
        </figcaption>
      </figure>

      <figure>
        <img src="spheres1024_16.png" width="500px" />
        <figcaption>
          Spheres generated with max_ray_depth = 5, 1024 samples per pixel, and
          16 rays per light
        </figcaption>
      </figure>

      <p>
        From the two examples above, we can see that indirect lighting doesn't
        contribute much until the scene becomes more complex. For example, the
        blob shows no light bleeding because it is the only object in the scene.
        In contrast, the spheres exhibit beautiful color blending on the roof,
        floor, and the spheres themselves. These subtle details make the scene
        appear much more realistic and lively.
      </p>

      <figure>
        <img src="spheres_1024_16_directonly.png" width="500px" />
        <figcaption>
          Spheres generated with s=1024 and l=16, but with direct lighting only.
        </figcaption>
      </figure>

      <figure>
        <img src="spheres_1024_16_indirectonly.png" width="500px" />
        <figcaption>
          Spheres generated with s=1024 and l=16, but with indirect lighting
          only.
        </figcaption>
      </figure>

      <p>
        Similar to before, with only direct lighting, the scene appears
        extremely bland and flat. In contrast, the scene with global
        illumination feels much more alive, with details brought out far more
        clearly.
      </p>

      <div style="display: flex; flex-direction: column; align-items: center">
        <table
          style="width: 100%; text-align: center; border-collapse: collapse"
        >
          <tr>
            <td>
              <img src="bunny_m0_1024.png" width="400px" />
              <figcaption>Bunny with 0th bounce only</figcaption>
            </td>
            <td>
              <img src="bunny_m0o1_1024.png" width="400px" />
              <figcaption>Bunny with up to 0th bounce</figcaption>
            </td>
          </tr>
          <tr>
            <td>
              <img src="bunny_m1_1024.png" width="400px" />
              <figcaption>Bunny with 1st bounce only (direct)</figcaption>
            </td>
            <td>
              <img src="bunny_m1o1_1024.png" width="400px" />
              <figcaption>Bunny with up to 1st bounce (direct)</figcaption>
            </td>
          </tr>
          <tr>
            <td>
              <img src="bunny_m2_1024.png" width="400px" />
              <figcaption>Bunny with 2nd bounce only</figcaption>
            </td>
            <td>
              <img src="bunny_m2o1_1024.png" width="400px" />
              <figcaption>Bunny with up to 2nd bounce</figcaption>
            </td>
          </tr>
          <tr>
            <td>
              <img src="bunny_m3_1024.png" width="400px" />
              <figcaption>Bunny with 3rd bounce only</figcaption>
            </td>
            <td>
              <img src="bunny_m3o1_1024.png" width="400px" />
              <figcaption>Bunny with up to 3rd bounce</figcaption>
            </td>
          </tr>
          <tr>
            <td>
              <img src="bunny_m4_1024.png" width="400px" />
              <figcaption>Bunny with 4th bounce only</figcaption>
            </td>
            <td>
              <img src="bunny_m4o1_1024.png" width="400px" />
              <figcaption>Bunny with up to 4th bounce</figcaption>
            </td>
          </tr>
          <tr>
            <td>
              <img src="bunny_m5_1024.png" width="400px" />
              <figcaption>Bunny with 5th bounce only</figcaption>
            </td>
            <td>
              <img src="bunny_m5o1_1024.png" width="400px" />
              <figcaption>Bunny with up to 5th bounce</figcaption>
            </td>
          </tr>
        </table>
      </div>

      <p>
        Looking at the second and third bounces above, they may seem
        insignificant at first, but the color blending on the bunny comes
        primarily from these two bounces. They are the main contributors to
        indirect lighting, as anything beyond three bounces becomes too dark to
        have a noticeable effect.
      </p>

      <div style="display: flex; flex-direction: column; align-items: center">
        <table
          style="width: 100%; text-align: center; border-collapse: collapse"
        >
          <tr>
            <td>
              <img src="bunny_m0o1_1024.png" width="400px" />
              <figcaption>
                Bunny with up to 0th bounce & Russian Roulette
              </figcaption>
            </td>
            <td>
              <img src="bunny_m1o1_1024.png" width="400px" />
              <figcaption>
                Bunny with up to 1st bounce (direct) & Russian Roulette
              </figcaption>
            </td>
          </tr>
          <tr>
            <td>
              <img src="bunny_m2o1_1024.png" width="400px" />
              <figcaption>
                Bunny with up to 2nd bounce & Russian Roulette
              </figcaption>
            </td>
            <td>
              <img src="bunny_m3o1_1024.png" width="400px" />
              <figcaption>
                Bunny with up to 3rd bounce & Russian Roulette
              </figcaption>
            </td>
          </tr>
          <tr>
            <td>
              <img src="bunny_m4o1_1024.png" width="400px" />
              <figcaption>
                Bunny with up to 4th bounce & Russian Roulette
              </figcaption>
            </td>
            <td>
              <img src="bunny_m5o1_1024.png" width="400px" />
              <figcaption>
                Bunny with up to 5th bounce & Russian Roulette
              </figcaption>
            </td>
          </tr>
          <tr>
            <td>
              <img src="bunny_m100o1_1024.png" width="400px" />
              <figcaption>
                Bunny with up to 100th bounce & Russian Roulette
              </figcaption>
            </td>
          </tr>
        </table>
      </div>

      <p>
        Although the photos above look very similar to those without the Russian
        roulette terminator, the difference lies in the time they took to
        render. To achieve roughly similar results in the 5-bounce image, the
        normal algorithm took over 50 minutes, while the Russian roulette
        version took about 20 minutes. This speed-up is due to random
        termination, which resulted in far fewer iterations.
      </p>

      <p>
        The images below show the bench scene rendered at various
        samples-per-pixel rates. For each render, the light ray count was
        clamped to 4 and the max_ray_depth was set to 5. As the sample rate
        increases, the noise gradually decreases. By 1024 samples, the image
        appears practically noiseless.
      </p>

      <div style="display: flex; flex-direction: column; align-items: center">
        <table
          style="width: 100%; text-align: center; border-collapse: collapse"
        >
          <tr>
            <td>
              <img src="bench1_4.png" width="400px" />
              <figcaption>Bench with 1 sample per pixel</figcaption>
            </td>
            <td>
              <img src="bench2_4.png" width="400px" />
              <figcaption>Bench with 2 samples per pixel</figcaption>
            </td>
          </tr>
          <tr>
            <td>
              <img src="bench4_4.png" width="400px" />
              <figcaption>Bench with 4 samples per pixel</figcaption>
            </td>
            <td>
              <img src="bench8_4.png" width="400px" />
              <figcaption>Bench with 8 samples per pixel</figcaption>
            </td>
          </tr>
          <tr>
            <td>
              <img src="bench16_4.png" width="400px" />
              <figcaption>Bench with 16 samples per pixel</figcaption>
            </td>
            <td>
              <img src="bench64_4.png" width="400px" />
              <figcaption>Bench with 64 samples per pixel</figcaption>
            </td>
          </tr>
          <tr>
            <td>
              <img src="bench1024_4.png" width="400px" />
              <figcaption>Bench with 1024 samples per pixel</figcaption>
            </td>
          </tr>
        </table>
      </div>

      <h2>Part 5: Adaptive Sampling</h2>

      <p>
        Adaptive sampling is a method used to avoid wasting time on pixels that
        have already reached a certain noise threshold. Monte Carlo integration
        produces random noise whose variance decreases proportionally to 1/N1/N,
        where NN is the number of samples. Some pixels converge to a stable
        value much faster than others; in such cases, the renderer can stop
        sampling those pixels and redirect effort toward pixels that still
        require more work. This approach produces a clean, high-quality image in
        less rendering time.
      </p>

      <p>
        My implementation works by taking samples in batches for each pixel (set
        to 64 in the examples below). After each batch, I calculate the mean
        brightness using the illuminance and determine the standard deviation to
        compute the 95% confidence interval width. If the relative noise falls
        below the maxTolerance (set to 0.05), sampling for that pixel stops, and
        the renderer moves on to the next pixel. Finally, the averaged color and
        the actual sample count are stored.
      </p>

      <div style="display: flex; flex-direction: column; align-items: center">
        <table
          style="width: 100%; text-align: center; border-collapse: collapse"
        >
          <tr>
            <td>
              <img src="bunny5.png" width="400px" />
              <figcaption>
                Bunny with 2048 max samples per pixel and adaptive sampling
              </figcaption>
            </td>
            <td>
              <img src="bunny5_rate.png" width="400px" />
              <figcaption>Scene Sampling rate</figcaption>
            </td>
          </tr>
        </table>
      </div>

      <div style="display: flex; flex-direction: column; align-items: center">
        <table
          style="width: 100%; text-align: center; border-collapse: collapse"
        >
          <tr>
            <td>
              <img src="spheres5.png" width="400px" />
              <figcaption>
                Spheres with 2048 max samples per pixel and adaptive sampling
              </figcaption>
            </td>
            <td>
              <img src="spheres5_rate.png" width="400px" />
              <figcaption>Scene Sampling rate</figcaption>
            </td>
          </tr>
        </table>
      </div>

      <p>
        In the photos above, we can see that areas with soft shadows required
        significantly more sampling. This aligns with our earlier observation
        that the bench had the most noise underneath—precisely where the soft
        shadows were. In contrast, regions such as the walls and floor, where
        there is little visual complexity, required relatively few samples to
        converge.
      </p>
    </div>
  </body>
</html>
