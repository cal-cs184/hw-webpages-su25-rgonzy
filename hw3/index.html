<html>
  <head>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default"></script>
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap"
      rel="stylesheet"
    />
    <style>
      h1 {
        text-align: center;
      }

      .container {
        margin: 0 auto;
        padding: 60px 20%;
      }

      figure {
        text-align: center;
      }

      img {
        display: inline-block;
      }

      h1 {
        color: rgb(255, 255, 255);
      }

      h2 {
        color: rgb(255, 255, 255);
      }

      h3 {
        color: rgb(171, 241, 255);
      }

      body {
        font-family: "Inter", sans-serif;
        text-align: center;
        color: #f3d0f5;
        background-color: #000000;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <h1>CS184/284A Spring 2025 Homework 3 Write-Up</h1>
      <div style="text-align: center">Names: Ruben Gonzalez</div>

      <br />

      Link to webpage:
      <a
        href="https://cal-cs184.github.io/hw-webpages-su25-rgonzy/hw3/index.html"
        >WebPage</a
      >
      Link to GitHub repository:
      <a href="https://github.com/cal-cs184/hw-pathtracer-updated-gonzyr.git"
        >Repo</a
      >

      <!-- <figure>
        <img
          src="cornell.png"
          alt="Cornell Boxes with Bunnies"
          style="width: 70%"
        />
        <figcaption>You can add images with captions!</figcaption>
      </figure> -->

      <!--
		We've already added one heading per part, to make your write-up as navigable when grading. Please fit your write-up within these sections!
		-->

      <h2>Overview</h2>
      Give a high-level overview of what you implemented in this homework. Think
      about what you've built as a whole. Share your thoughts on what
      interesting things you've learned from completing the homework.

      <h2>Part 1: Ray Generation and Scene Intersection</h2>

      <p>
        To generate rays, I first convert the sensor sample coordinates into
        camera space by linearly interpolating between the camera-space edges.
        Next, for each pixel, I apply anti-aliasing by jittering the sample
        position and normalizing it using the dimensions of the sample buffer. I
        then generate a ray from the camera origin through this normalized
        point, call <code>est_radiance_global_illumination</code> along that
        ray, and add the result to a running sum. After tracing all samples, I
        divide the sum by the number of samples to obtain the final pixel color.
      </p>
      <p>
        With the basic ray tracer in place, I extended it to handle triangles by
        first testing the ray against the triangle's bounding box and, if that
        succeeds, running the Möller-Trumbore intersection. I begin by computing
        the two edge vectors E₁ and E₂ of the triangle and then form the cross
        product P between the ray direction D and E₂. I take the dot product of
        E₁ and P to get the determinant det; if det is nearly zero, the ray is
        effectively parallel to the triangle and I immediately return false.
        Otherwise, I subtract the triangle's first vertex A from the ray origin
        O to form T, use T⋅P / det to obtain the first barycentric coordinate u,
        and return false if u lies outside [0, 1]. Next, I form Q as the cross
        product of T and E₁, compute the second barycentric coordinate v as D⋅Q
        / det, and again bail out if v falls outside [0, 1] or if u + v exceeds
        1. Finally, I compute the ray parameter t by taking E₂⋅Q / det, and if t
        is outside the ray's allowed range I return false; otherwise I update
        ray.max_t to t, record the primitive ID and BSDF, compute the surface
        normal by area-weighted interpolation of the triangle's vertex normals
        using u and v, store that in the intersection, and return true.
        <br />
        <br />
        The sphere intersection follows a similar pattern. I compute the
        coefficients a, b, and c for the quadratic equation, then check whether
        the discriminant (b*b - 4*a*c) is non-negative. If it is, I solve for
        the two roots and verify whether either lies between ray.min_t and
        ray.max_t. When a valid root is found, I set ray.max_t to the smaller
        root, compute the surface normal as the normalized vector from the
        sphere;s center to the hit point, fill the intersection record
        (primitive ID, BSDF, and this normal), and return true.
      </p>

      <p>
        In the photos below, you can see the result of my pathtracer up to this
        point.
      </p>

      <div style="display: flex; flex-direction: column; align-items: center">
        <table
          style="width: 100%; text-align: center; border-collapse: collapse"
        >
          <tr>
            <td>
              <img src="part1.1.png" width="400px" />
              <figcaption>Spheres</figcaption>
            </td>
            <td>
              <img src="part1.2.png" width="400px" />
              <figcaption>Coil</figcaption>
            </td>
          </tr>
          <tr>
            <img src="part1.3.png" width="400px" />
            <figcaption>Bunny</figcaption>
          </tr>
        </table>
      </div>

      <!-- <p>
        Here is an example 2x2 gridlike structure using an HTML table. Each
        <b>tr</b> is a row and each <b>td</b> is a column in that row. You might
        find this useful for framing and showing your result images in an
        organized fashion.
      </p>
      <div style="display: flex; flex-direction: column; align-items: center">
        <table
          style="width: 100%; text-align: center; border-collapse: collapse"
        >
          <tr>
            <td style="text-align: center">
              <img src="cornell.png" width="400px" />
              <figcaption>Caption goes here.</figcaption>
            </td>
            <td style="text-align: center">
              <img src="cornell.png" width="400px" />
              <figcaption>Caption goes here.</figcaption>
            </td>
          </tr>
          <tr>
            <td style="text-align: center">
              <img src="cornell.png" width="400px" />
              <figcaption>Caption goes here.</figcaption>
            </td>
            <td style="text-align: center">
              <img src="cornell.png" width="400px" />
              <figcaption>Caption goes here.</figcaption>
            </td>
          </tr>
        </table>
      </div> -->

      <h2>Part 2: Bounding Volume Hierarchy</h2>
      <p>
        To build the BVH, I begin by initializing the structure and allocating
        space for every scene primitive. I then create the root node and
        recursively subdivide its primitives until each leaf contains at most
        max_leaf_size items. At each split, I choose the longest axis of the
        node's bounding box by comparing its diagonal's x, y, and z components.
        Along that axis, I form a small bounding box that encloses only the
        primitives' centroids and pick the split plane at its midpoint. I
        partition the primitives into two groups based on which side of this
        plane their centroid falls on and pass the midpoint iterator into the
        left and right recursive calls. Finally, to avoid edge cases when all
        centroids lie on one side, I fall back to dividing the list evenly in
        half.
      </p>

      <figure>
        <img src="part2.4.png" width="500px" />
        <figcaption>Cow generated using BVH</figcaption>
      </figure>

      <figure>
        <img src="part2.3.png" width="500px" />
        <figcaption>Dragon generated using BVH</figcaption>
      </figure>

      <p>
        The speed-up from BVH is an incredible feat. Below are some time
        comparisons.
      </p>

      <figure>
        <img src="part2.1.png" width="800px" />
        <figcaption>CBLucy generated in .2045s compared to 52s</figcaption>
      </figure>

      <figure>
        <img src="part2.2.png" width="500px" />
        <figcaption>Beast generated in .1375s compared to 37s</figcaption>
      </figure>

      <figure>
        <img src="part2.5.png" width="500px" />
        <figcaption>Walle generated in .3492s compared to 83s</figcaption>
      </figure>

      <p>
        Rendering times were significantly reduced when using BVH acceleration
        compared to rendering without it. In scenes with moderately complex
        geometries, the version without BVH had to check every object for each
        ray, resulting in noticeably longer render times. With BVH, objects are
        grouped into bounding boxes within a tree structure, allowing the
        renderer to skip entire groups of objects when a ray misses a bounding
        box. This reduces the number of intersection tests from O(N) to
        approximately O(log N), leading to a substantial performance
        improvement. The difference in times clearly demonstrates the
        effectiveness of BVH in optimizing ray tracing workloads.
      </p>

      <h2>Part 3: Direct Illumination</h2>

      <p>
        To estimate direct lighting, I implemented two functions: one that
        samples directions uniformly over the hemisphere, and another that
        samples only from light sources. The hemisphere-based function generates
        a total of lights * ns_area_light samples. For each sample, it creates a
        ray originating from the surface hit point and directed toward a
        randomly chosen point on the hemisphere above the surface normal. If
        this ray intersects another object, the incoming radiance from that
        direction is computed using the BSDF and the emission at the
        intersection point. This contribution is added to a running sum. After
        all samples are processed, the accumulated radiance is divided by the
        number of samples and the probability density function (PDF) for uniform
        hemisphere sampling to produce the final estimate.
      </p>

      <p>
        Direct lighting samples points on light sources and casts shadow rays
        from the hit point to these sampled light positions. If the shadow ray
        reaches the light unblocked, it evaluates the rendering equation for
        that light contribution. This approach is efficient because it only
        samples directions that potentially contribute light, unlike hemisphere
        sampling which may waste samples on directions with no illumination. The
        final result properly weights each sample by its probability density to
        maintain an unbiased estimate.
      </p>

      <p>YOU ALREADY HAVE PICTURES JUST PUT THAT TOO</p>

      <p>
        Hemisphere sampling uses random directions (often cosine-weighted)
        across the hemisphere above a surface point. While unbiased, it wastes
        samples on directions where little light arrives. Importance sampling
        strategically biases the random sampling toward directions that
        contribute more to the final result - like toward light sources or based
        on the BRDF. This reduces noise because more samples hit areas that
        actually matter for the lighting calculation. It's still random, but the
        randomness is weighted to be more effective. Both methods eventually
        converge to the correct result, but importance sampling typically needs
        fewer samples to achieve the same quality.
      </p>

      <h2>Part 4: Global Illumination</h2>
      <p>
        To achieve global (indirect) illumination, I implemented a recursive
        path tracing algorithm that begins with the first bounce. After
        computing the initial bounce, I used Russian roulette to
        probabilistically terminate the path. If the path continues, I sample
        the BSDF at the intersection point to generate a new ray direction and
        trace it further. If this new ray intersects another object, the process
        repeats recursively until the maximum ray depth is reached or the path
        is terminated. At each step, the incoming radiance is weighted by the
        BSDF and accumulated, forming a running sum that approximates the total
        indirect lighting contribution.
      </p>

      <p>
        Pick one scene and compare rendered views first with only direct
        illumination, then only indirect illumination. Use 1024 samples per
        pixel. (You will have to edit
        PathTracer::at_least_one_bounce_radiance(...) in your code to generate
        these views.) """STILL HAVENT DONE THIS< BUT RUN THIS ON HOME PC WHILE
        YOU STUDY"""
      </p>

      <h2>Part 5: Adaptive Sampling</h2>
      Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod
      tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim
      veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea
      commodo consequat. Duis aute irure dolor in reprehenderit in voluptate
      velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat
      cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id
      est laborum.
    </div>
  </body>
</html>
